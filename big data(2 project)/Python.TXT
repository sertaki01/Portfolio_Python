# Subject 4 (1)
# Download the neccesary libraries
#This library is used for fetching datasets from the UCI Machine Learning Repository.
from ucimlrepo import fetch_ucirepo
# This one is used to split the dataset into training and testing sets.
from sklearn.model_selection import train_test_split
#This is a classifier based on decision tree algorithms, plot_tree is used for visualizing decision trees, export_text is used for generating a text repressentation of deccison tree rules
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
#Metrics for evaluating the model's performance.
from sklearn.metrics import confusion_matrix, accuracy_score
# Data manipulator library, we use it to create data frame 
import pandas as pd
#used for preproccesing the data, OneHotEncoder we use for transphorming categorical variables into numeric ones
from sklearn.preprocessing import OneHotEncoder
#SimpleImputer is used to fill missing values in the dataset, and ColumnTransformer allows us to manipulate certain columns 
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
# Library for plotting graphs
import matplotlib.pyplot as plt

# fetches the Mushroom dataset from the UCI Machine Learning Repository, id=73 refers to the datasets id in UCI 
mushroom = fetch_ucirepo(id=73) 
  
# This variable represents the features or independent variables of the dataset, each row in X corresponds to a different mushroom, and each column represents a specific feature 
X = mushroom.data.features 
# This variable represents the target or dependent variable of the dataset, in this case the "poisonous" variable, Each element in Y corresponds to the target label for the corresponding row in X
Y = mushroom.data.targets 
  
# metadata 
print(mushroom.metadata) 
  
# variable information 
print(mushroom.variables) 
#combine features X and target Y into a single dataframe
mushrooms = pd.concat([X, Y], axis=1)

# Handle missing values, by filling the missing values with the most common value, the lambda function takes each column (x) of the DataFrame, and for each missing value(na), it fills it with the most frequent value in that column, computed by the function inside the x.fillna()
mushrooms_imputed = mushrooms.apply(lambda x: x.fillna(x.value_counts().index[0]))
# One-hot encoding for categorical columns, creating dummies for all categories and drop the first variable to avoid multicollinearity
mushrooms_encoded = pd.get_dummies(mushrooms_imputed, drop_first=True)

# Split the data into training(80%) and testing sets(20%), poisonous_p is the dummy for poisonous(=0 if edible, =1 if poisonous)
X_train, X_test, Y_train, Y_test = train_test_split(
    mushrooms_encoded.drop("poisonous_p", axis=1),
    mushrooms_encoded["poisonous_p"],
    test_size=0.2,
    random_state=123
)

# Create a Decision Tree Classifier, random_sta=123 refers to the seed for the random number generator, the specific number is arbitrary any integer would do. This ensure that  the same seed will lead to the same random processes during the training of the decision tree, providing consistency in the results
mushroom_tree = DecisionTreeClassifier(random_state=123)
#train it on the train data set
mushroom_tree.fit(X_train, Y_train)

# This line uses the trained decision tree classifier, to make predictions on the test set, storing those predictions in the Y-pred variable
Y_pred = mushroom_tree.predict(X_test)

#The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It takes four entries True Positive, True Negative, False Positive and False Negative basically telling us where the model made invalid and where accurate predictions 
conf_matrix = confusion_matrix(Y_test, Y_pred)
#The accuracy is the ratio of correctly predicted instances to the total instances in the test set
accuracy = accuracy_score(Y_test, Y_pred)

# Display the confusion matrix and accuracy
print("Confusion Matrix:")
print(conf_matrix)
print("Accuracy:", accuracy)

# Visualize the decision tree (text representation)
tree_rules = export_text(mushroom_tree, feature_names=list(X_train.columns))
print(tree_rules)

#visualize decision tree as plot, replacing labels 

plt.figure(figsize=(12, 8))
plot_tree(mushroom_tree, feature_names=list(X_train.columns), class_names=["edible", "poisonous"], filled=True, rounded=True)
plt.show()




# Project 4 (2)
# Imports the pandas library, which is used for data manipulation and analysis
import pandas as pd
#Imports the log2 function from the math module. This will be used for calculating logarithms with base 2
from math import log2
#This function is used to fetch datasets from the UCI Machine Learning Repository.
from ucimlrepo import fetch_ucirepo
# fetch dataset( data sets id_number, in UCI Machine Learning Respitory, is 73)
mushroom = fetch_ucirepo(id=73) 
  
# This variable represents the features or independent variables of the dataset, each row in X corresponds to a different mushroom, and each column represents a specific feature 
X = mushroom.data.features 
# This variable represents the target or dependent variable of the dataset, in this case the "poisonous" variable, Each element in Y corresponds to the target label for the corresponding row in X
Y = mushroom.data.targets 
  
# metadata 
print(mushroom.metadata) 
  
# variable information 
print(mushroom.variables) 
#combine features X and target Y into a single dataframe
mushrooms = pd.concat([X, Y], axis=1)
# Handle missing values, by filling the missing values with the most common value, the lambda function takes each column (x) of the DataFrame, and for each missing value(na), it fills it with the most frequent value in that column, computed by the function inside the x.fillna()
mushrooms_imputed = mushrooms.apply(lambda x: x.fillna(x.value_counts().index[0]))

#subsetting_data, creating data set with just the first 30 obs. of the original data set
data_subset = mushrooms_imputed.head(30)

# Defines a function that calculates the entropy of a set based on the frequencies of the positive ('e' - edible) and negative ('p' - poisonous) classes in the set.
def calculate_entropy(s):
    p_plus = s[s == 'e'].count() / len(s)
    p_minus = s[s == 'p'].count() / len(s)
    entropy = -p_plus * log2(p_plus) - p_minus * log2(p_minus) if p_plus > 0 and p_minus > 0 else 0
    return entropy

#Defines a function that calculates the weighted entropy after splitting the data on a given attribute
#unique_values stores the unique values that the specified attribute can take within the given dataset
def calculate_weighted_entropy(data, attribute, target):
    unique_values = data[attribute].unique()
    entropy_weighted = 0
#The function iterates through each unique value of the specified attribute, and selects the subset where the data values is found It then calculates the weighted entropy for that subset and adds it to the overall entropy(entropy_weighted), the weight is determined by the relative size of the subset compared to the original dataset
    for value in unique_values:
        subset = data[data[attribute] == value][target]
        entropy_weighted += len(subset) / len(data) * calculate_entropy(subset)

    return entropy_weighted

# calculate the entropy gain when splitting a dataset based on a specified attribute
def calculate_entropy_gain(data, attribute, target):
#calculates the entropy of the original dataset with respect to the target variable
    entropy_original = calculate_entropy(data[target])
#calculates the weighted entropy after splitting the dataset based on the specified attribute
    entropy_weighted = calculate_weighted_entropy(data, attribute, target)
#s the difference between the original entropy and the weighted entropy after splitting, generally a higher entropy gain indicates a more effective split in terms of reducing uncertainty about the target variable
    entropy_gain = entropy_original - entropy_weighted
    return entropy_gain

# Calculate entropy gains for the "habitat" attribute
entropy_gains_habitat = {}

for attribute in ['habitat']:
    entropy_gains_habitat[attribute] = calculate_entropy_gain(data_subset, attribute, 'poisonous')

# Print entropy gains
entropy = calculate_entropy(data_subset['poisonous'])
entropy_weighted = calculate_weighted_entropy(data_subset, 'habitat', 'poisonous')
print("Entropy of set:", entropy)
print("Weighted entropy after splitting at habitat:", entropy_weighted)

for attribute, entropy_gain in entropy_gains_habitat.items():
    print(f"Entropy Gain for {attribute}:", entropy_gain)




# Project 4 (3)
# Importing necessary libraries
# This one is used to split the dataset into training and testing sets.
from sklearn.model_selection import train_test_split
# imports the Gaussian Naive Bayes algorithm (probabilistic classifier)
from sklearn.naive_bayes import GaussianNB
#Metrics for evaluating the model's performance.
from sklearn.metrics import confusion_matrix, accuracy_score
# Data manipulator library, we use it to create data frame 
import pandas as pd
#This library is used for fetching datasets from the UCI Machine Learning Repository.
from ucimlrepo import fetch_ucirepo

# fetches the Mushroom dataset from the UCI Machine Learning Repository, id=73 refers to the datasets id in UCI 
mushroom = fetch_ucirepo(id=73)
# This variable represents the features or independent variables of the dataset, each row in X corresponds to a different mushroom, and each column represents a specific feature 
X = mushroom.data.features
# This variable represents the target or dependent variable of the dataset, in this case the "poisonous" variable, Each element in Y corresponds to the target label for the corresponding row in X
Y = mushroom.data.targets

# Combine features X and target Y into a single dataframe
mushrooms = pd.concat([X, Y], axis=1)

# Handle missing values by filling them with the most common value
mushrooms_imputed = mushrooms.apply(lambda x: x.fillna(x.value_counts().index[0]))

# One-hot encoding for categorical columns, creating dummies for all categories and drop the first variable to avoid multicollinearity
mushrooms_encoded = pd.get_dummies(mushrooms_imputed, drop_first=True)

# One-hot encoding for categorical columns, creating dummies for all categories and drop the first variable to avoid multicollinearity
X_train, X_test, Y_train, Y_test = train_test_split(
    mushrooms_encoded.drop("poisonous_p", axis=1),
    mushrooms_encoded["poisonous_p"],
    test_size=0.2,
    random_state=123
)

# Create a Naive Bayes (Gaussian) Classifier
naive_bayes_classifier = GaussianNB()
#Trains the Naive Bayes classifier on the training data
naive_bayes_classifier.fit(X_train, Y_train)

# Predict probabilities for both training and testing sets
probs_train = naive_bayes_classifier.predict_proba(X_train)
probs_test = naive_bayes_classifier.predict_proba(X_test)

# Display confusion matrix and accuracy for the Naive Bayes model
#Implement Naive_bayes on the test set and store results in variable (Y_pred_nb)
Y_pred_nb = naive_bayes_classifier.predict(X_test)
conf_matrix_nb = confusion_matrix(Y_test, Y_pred_nb)
accuracy_nb = accuracy_score(Y_test, Y_pred_nb)
print("Naive Bayes Confusion Matrix:")
print(conf_matrix_nb)
print("Naive Bayes Accuracy:", accuracy_nb)

